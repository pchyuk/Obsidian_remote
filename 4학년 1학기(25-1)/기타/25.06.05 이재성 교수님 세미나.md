# ChatGPT와 한국어
## ChatGPT는 한국어에 최적인가?
### 1. 영어 질문과 한국어 질문의 차이
- 한국어 질문과 답변, 영어 질문과 답변의 차이
	- 한국어 답변의 정확도가 더 낮다
	- 한국어 답변이 보다 감정적

- 답변 차이의 원인
	- 주 원인
		- 학습데이터의 크기 차이로 인한
		- 기본적인 학습의 부족
		- 정보의 부족 및 편향
	- 부 원인
		- 후처리(fine tuning) RLHF에서의 한국식 학습 (한국식 선호)

---
### 2. ChatGPT 모델의 학습과 생성
- 학습 데이터
	- GPT-3
		- 약 570GB
		- **영어 93%**
		- 한국어 **0.02%** 미만
	- GPT-4
		- 미공개
		- 다양한 언어에서 성능 향상

- 한국어 학습 데이터의 부족 영향
	- 한국어 토큰의 비효율적 표현
	- 한국어 답변의 부실

---
#### 사전 학습 (pre-training)
- 사전 학습
	- 싼 비용의 대규모 원시 텍스트(raw text)를 이용하여 학습 (self-supervised learning)
	- 모델이 일반적 언어를 처리할 수 있도록 초기화
	- 거대 모델의 파라미터 초기값은 성능에 매우 중요
	- 사전 학습후 다른 세부 작업에 대한 정밀 조정 학습으로 성능 향상

- 학습 데이터 양과 사전 학습
	- 학습 데이터가 많을수록 사전 학습이 효과적

- GPT 학습 방법
	- 다음 단어 예측
	- 일반 텍스트에서 앞의 단어를 보여주고, 다음 단어를 예측하도록 학습

---
#### GPT 모델
- GPT: Generative Pre-Training
	- 단방향 Transformer decoder 학습 모델
		- auto regressive (자기 회귀 모델)

- Context window의 한계
	- Prompt, 질문, 답변이 Context window 내에 들어와야 함

---
#### Context window
- GPT context window size
	- GPT-3.5 turbo: 4K
	- GPT-4: 8K
	- GPT-4 turbo: 32K (기본 모드)

- 구성
	- System Prompt: 시스템의 안내
	- User Prompt: 사용자 질문
	- Chat history: 사용자와 다화한 내용 기롤
	- 답변: 새로 출력할 여분의 칸
	- ***전체 길이를 고려하여 각각의 prompt 작성 필요***

---
### 3. 토큰화와 문제점
- Context window 내의 토큰 수
	- 실제 트랜스포머의 입력 및 출력 크기는 토큰수로 제한됨(단어수가 아님)
	- 입출력시 토큰은 단어에서 변환되거나 단어로 변환됨

#### 토큰화
- 토큰화
	- 단어를 통계에 기반하여 부분 단어(토큰)로 분리하여 표기
	- 예: lovely, friendly, beautiful -> love+ly, friend+ly, beautiful
	- 예: 미등록어 beautifully는 beautiful+ly로 분리하면 처리 가능
	- 최소한의 토큰화로 최대한의 미등록 단어 표시
	- 중요 빈도 단어를 더 크게 토큰화

- 부작용
	- 저빈도 단어의 경우, 최소 단위인 글자 단위(혹은 바이트 단위)로 분리되어 context window를 많이 차지하게 됨
	- => 충분한 문맥을 보거나, 생성할 수 없게 됨

---
#### Byte Pair Encoding (BPE)
- Byte Pair Encoding
	- 사전을 이용해 문자열을 최소 길이로 압축
	- 사전 크기는 증가하지만, |문자열| >> |사전|

- Byte Pair Encoding 방법
	1. 말뭉치의 모든 문장을 공백으로 나눔
	2. 각 단어의 빈도를 세어 표로 작성
	3. unigram을 어휘 사전에 저장
	4. 단어의 각 글자를 bigram으로 형성 후 빈도 계산
	5. **최고 빈도의 bigram을 어휘 사전에 저장**
	6. 각 단어에 있는 그 bigram을 한 단위(새로운 한 글자)로 대체
	7. 다시 빈도 세어 표로 작성하고 반복
	8. 일정 어휘 사전의 크기가 될 때 정지

#### Word Piece Encoding (WPE)
#### Sentence Piece Encoding (SPE)
---
#### ChatGPT에서의 한국어 토큰화 문제점
- 영어 단어에 비해 매우 빈도가 낮아 중요도 낮음
- 한글 음절로 분리되어 토큰화되는 경향
- (빈도가 적을 경우, 최악으로는 바이트 단위로 토큰화)

- 중요 단위 무시
- 많은 Context를 볼 수 없어 답변 부정확

---
### 4. 한국어 순화 학습
#### 언어 모델의 순화 (Alignment tuning)
- 순화 정밀 조정
	- 언어 모델의 출력이 윤리적, 정치적, 사회적으로 편향되거나 독성이 있고 사람의 기준에 맞지 않을 때, 이를 수정하는 작업
	- 사람의 판단에 따라 기준이 다를 수 있으니 일반적으로 인정하는 방향으로 학습
	- 문화적 차이에 따른 조정

#### 언어 모델의 순화 - RLHF
- 1단계
	- 프롬프트(질문)에 대한 바람직한 답변으로 GPT 언어 모델을 정밀 조정
	- 레이블 작업자가 답변 생성(SFT: Supervised Fine Tuning)
- 2단계
	- 조정된 언어 모델로 프롬프트에 여러 답변
		- 예: A, B, C, D 답변 출력
	- 레이블 작업자가 답변 선호도 정렬 (human feedback)
		- 예: D > C > A = B
	- 답변 선호도를 InstructGPT (RM)가 학습
- 3단계
	- 새로운 대량의 프롬프트에 대한 답변을 GPT 언어 모델이 생성하고 이를 InstructGPT (RM)가 평가
	- 평가 결과를 GPT 언어 모델에 반영 (Reignforcement Learning, 강화학습)

---
### 5. 맺음말
- 영어 중심의 ChatGPT에서 한국어 문제점
	- 한국어 토큰 분리가 과도
	- 문맥 정보 및 토큰 의미 상실 우려
	- 한국어 학습 데이터 부족
	- RLHF에서 한국어에 맞는 순화 학습 필요

- 한국어에 맞는 LLM(Chatbot)을 개발하려면
	- 한국어 중심의 tokenizer 개발이 필수
	- 한국어 데이터를 중심으로 학습
	- 대량으로 좋은 품질의 한국어 데이터 구축하여 학습에 사용

- 한국어 LLM
