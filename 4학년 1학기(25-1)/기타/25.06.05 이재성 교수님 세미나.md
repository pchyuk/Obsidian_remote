# ChatGPT와 한국어
## ChatGPT는 한국어에 최적인가?
### 1. 영어 질문과 한국어 질문의 차이
- 한국어 질문과 답변, 영어 질문과 답변의 차이
	- 한국어 답변의 정확도가 더 낮다
	- 한국어 답변이 보다 감정적

- 답변 차이의 원인
	- 주 원인
		- 학습데이터의 크기 차이로 인한
		- 기본적인 학습의 부족
		- 정보의 부족 및 편향
	- 부 원인
		- 후처리(fine tuning) RLHF에서의 한국식 학습 (한국식 선호)

---
### 2. ChatGPT 모델의 학습과 생성
- 학습 데이터
	- GPT-3
		- 약 570GB
		- **영어 93%**
		- 한국어 **0.02%** 미만
	- GPT-4
		- 미공개
		- 다양한 언어에서 성능 향상

- 한국어 학습 데이터의 부족 영향
	- 한국어 토큰의 비효율적 표현
	- 한국어 답변의 부실

#### 사전 학습 (pre-training)
- 사전 학습
	- 싼 비용의 대규모 원시 텍스트(raw text)를 이용하여 학습 (self-supervised learning)
	- 모델이 일반적 언어를 처리할 수 있도록 초기화
	- 거대 모델의 파라미터 초기값은 성능에 매우 중요
	- 사전 학습후 다른 세부 작업에 대한 정밀 조정 학습으로 성능 향상

- 학습 데이터 양과 사전 학습
	- 학습 데이터가 많을수록 사전 학습이 효과적

- GPT 학습 방법
	- 다음 단어 예측
	- 일반 텍스트에서 앞의 단어를 보여주고, 다음 단어를 예측하도록 학습

#### GPT 모델
- GPT: Generative Pre-Training
	- 단방향 Transformer decoder 학습 모델
		- auto regressive (자기 회귀 모델)

- Context window의 한계
	- Prompt, 질문, 답변이 Context window 내에 들어와야 함

#### Context window
- GPT context window size
	- GPT-3.5 turbo: 4K
	- GPT-4: 8K
	- GPT-4 turbo: 32K (기본 모드)

- 구성
	- System Prompt: 시스템의 안내
	- User Prompt: 사용자 질문
	- Chat history: 사용자와 다화한 내용 기롤
	- 답변: 새로 출력할 여분의 칸
	- ***전체 길이를 고려하여 각각의 prompt 작성 필요***

---
### 3. 토큰화와 문제점
- Context window 내의 토큰 수
	- 실제 트랜스포머의 입력 및 출력 크기는 토큰수로 제한됨(단어수가 아님)
	- 입출력시 토큰은 단어에서 변환되거나 단어로 변환됨



---
### 4. 한국어 순화 학습

---
### 5. 맺음말