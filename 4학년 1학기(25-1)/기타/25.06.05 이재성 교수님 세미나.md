# ChatGPT와 한국어
## ChatGPT는 한국어에 최적인가?
### 1. 영어 질문과 한국어 질문의 차이
- 한국어 질문과 답변, 영어 질문과 답변의 차이
	- 한국어 답변의 정확도가 더 낮다
	- 한국어 답변이 보다 감정적

- 답변 차이의 원인
	- 주 원인
		- 학습데이터의 크기 차이로 인한
		- 기본적인 학습의 부족
		- 정보의 부족 및 편향
	- 부 원인
		- 후처리(fine tuning) RLHF에서의 한국식 학습 (한국식 선호)

---
### 2. ChatGPT 모델의 학습과 생성
- 학습 데이터
	- GPT-3
		- 약 570GB
		- **영어 93%**
		- 한국어 **0.02%** 미만
	- GPT-4
		- 미공개
		- 다양한 언어에서 성능 향상

- 한국어 학습 데이터의 부족 영향
	- 한국어 토큰의 비효율적 표현
	- 한국어 답변의 부실

#### 사전 학습
- 사전 학습
	- 싼 비용의 대규모 원시 텍스트를 이용하여 학습
	- 모델이 일반적 언어를 처리할 수 있도록 초기화
	- 거대 모델의 파라미터 초기값은 성능에 매우 중요
	- 사전 학습후 다른 세부 작업에 대한 정밀 조정 하


---
### 3. 토큰화와 문제점

---
### 4. 한국어 순화 학습

---
### 5. 맺음말